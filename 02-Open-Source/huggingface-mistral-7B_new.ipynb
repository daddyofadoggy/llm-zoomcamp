{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d1c9fe-d648-4d5c-bf47-893566feb848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                                     Size    Used   Avail Capacity iused ifree %iused  Mounted on\n",
      "/dev/disk3s1s1                                926Gi    10Gi   802Gi     2%    425k  4.3G    0%   /\n",
      "devfs                                         205Ki   205Ki     0Bi   100%     708     0  100%   /dev\n",
      "/dev/disk3s6                                  926Gi    16Gi   802Gi     2%      16  8.4G    0%   /System/Volumes/VM\n",
      "/dev/disk3s2                                  926Gi   6.6Gi   802Gi     1%    1.3k  8.4G    0%   /System/Volumes/Preboot\n",
      "/dev/disk3s4                                  926Gi   5.3Mi   802Gi     1%      58  8.4G    0%   /System/Volumes/Update\n",
      "/dev/disk1s2                                  500Mi   6.0Mi   481Mi     2%       1  4.9M    0%   /System/Volumes/xarts\n",
      "/dev/disk1s1                                  500Mi   5.4Mi   481Mi     2%      29  4.9M    0%   /System/Volumes/iSCPreboot\n",
      "/dev/disk1s3                                  500Mi   2.4Mi   481Mi     1%      91  4.9M    0%   /System/Volumes/Hardware\n",
      "/dev/disk3s5                                  926Gi    90Gi   802Gi    11%    963k  8.4G    0%   /System/Volumes/Data\n",
      "map auto_home                                   0Bi     0Bi     0Bi   100%       0     0     -   /System/Volumes/Data/home\n",
      "/Users/ron/Downloads/Visual Studio Code.app   926Gi    51Gi   857Gi     6%    685k  9.0G    0%   /private/var/folders/wb/_77j3j256dlc5yrp7_0dxfg40000gn/T/AppTranslocation/AF3EBBEE-7AF8-47E4-BE7C-E7853555E271\n",
      "/dev/disk4s2                                  2.0Gi   1.8Gi   171Mi    92%     436  4.3G    0%   /Volumes/Docker\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b6e338-05d0-4673-b09e-0035cb047c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18428cac-8a73-4597-8276-23449e1fbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff0a65-bef1-4b7f-a683-df9a65c0baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d05676-7d2b-420e-8997-56cd3fe74d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a57a0-3222-4f99-abbd-78d128f84c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd165e7-4870-464a-b21a-4daa03c40451",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aed1ab-12f8-45e3-a859-bc93722cf9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e97cd39-ced9-4476-b0d6-a981f6d8026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb55daac-693e-404f-8464-1452db6f0bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/System/Volumes/Data/Users/ron/Documents/llm_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "434e5cc3-aacc-4b01-92de-a5f42514f9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "277736e3-f486-4ded-bd2e-478fc8857984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37df775c-4938-46c2-b6b6-d2a4e942a90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ron/Documents/github/myenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x1128ac9d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "import minsearch\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77edb598-3b26-49d2-b6e0-a2861083e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=3\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "043e7d69-6c55-47e9-9194-c5f490ffb9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c28808ec-6cb5-45a9-ac42-127da105eeef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x130f0ed30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15fc1c04-2c3b-4849-8d9e-a296e2fc2cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d34461-772d-41b3-bb5f-323b0e4fab32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3e606d-667c-42b7-a605-28afd4b919ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da3130a3-af4d-4eb3-bbd0-80bb35c811ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "413cdd86-5b3f-4740-83b2-0e77ed6e1be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2b64d-f0b5-4424-9dc3-05664924497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "#from transformers import pipeline\n",
    "\n",
    "#pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359305ed-0b51-494f-8f0e-33691e7b3039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc4c20e9-cf43-44d8-b55b-2140a193fb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654b0d71283947e68f01d0290aace49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fc2e7f557140d88cbcf91aa0fca2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f622df1ec574ed6b95309775769d2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0389cd50dabb4ae4a1e0c0c4ef5ccd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6f561ac9764cd58e55f2b4e48f8455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeedc4ee4be94271b4606714879135e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25e3185bbd14c5d81fe980cd06b8db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9507c60b94e4ce58cb84650cee631e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f3c00454fb416496086dea1fac86a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5579756db534effb413198bd74dc666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19aa6aaefe84d3081f0934d781c3e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\",device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9f0262a-04a4-4322-877c-ae8da2ec85ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c9fe330-2212-4241-ac21-5b8a211b33b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"{doc['question']}\\n{doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    response = generator(prompt, max_length=500, temperature=0.7, top_p=0.95, num_return_sequences=1)\n",
    "    response_final = response[0]['generated_text']\n",
    "    return response_final[len(prompt):].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19a7de5a-812f-4cc3-9705-5e8441680ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/ron/Documents/github/myenv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/ron/Documents/github/myenv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, you can still join the course.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"I just discovered the course. Can I still join it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fafbcba-5ea7-46e2-aed7-1789e77a2e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250eec69-b1e6-4889-8b36-ab2814d11909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d8f65-c2fb-4549-bed9-cad512a7b4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90070e03-ee56-4688-bf79-5c5ee1898d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142ee032-a7f5-4d5b-bc3f-0d0652d89972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09084e6d-1476-475b-9b0e-942c358cb8b0",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c7d809-48c5-46f4-8449-7af436c96fe8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
